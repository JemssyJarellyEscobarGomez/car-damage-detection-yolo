{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaci√≥n de herramientas de etiquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install labelme # Software de etiquetado que almacena informaci√≥n en archivos JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install labelme2yolo # Herramienta de conversi√≥n para el formato JSON de labelme a formato texto requerido por los modelos de detecci√≥n de objetos YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversi√≥n a formato YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta de entrenamiento\n",
    "dir_json_train='' # Incluya la direcci√≥n de la carpeta que contiene las etiquetas en archivos JSON separadas para el entrenamiento.\n",
    "!labelme2yolo --json_dir {dir_json_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta de validaci√≥n\n",
    "dir_json_val='' # Incluya la direcci√≥n de la carpeta que contiene las etiquetas en archivos JSON separadas para la validaci√≥n.\n",
    "!labelme2yolo --json_dir {dir_json_val}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "import gdown\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaci√≥n de recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.220 üöÄ Python-3.10.13 torch-2.1.1 CPU (AMD Ryzen 5 4500U with Radeon Graphics)\n",
      "Setup complete ‚úÖ (6 CPUs, 7.4 GB RAM, 228.9/476.2 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics # Instalaci√≥n de la biblioteca Ultralytics\n",
    "\n",
    "ultralytics.checks() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funci√≥n para determinar si tiene GPU Cuda para instalaci√≥n de PYTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'CUDA: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    try:\n",
    "        command = 'wmic path win32_videocontroller get caption'\n",
    "        device = subprocess.check_output(command, shell=True, universal_newlines=True)\n",
    "        print(f'Tarjeta Grafica: {device.strip()}. \\nAdvertencia: Debe utilizar la CPU para instalaci√≥n de PyTorch!. ')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'Error al ejecutar el comando: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bahnschrift Light'; font-size: 18px\">Direcci√≥n para instalar mediante el comando la versi√≥n de PyTorch en funci√≥n de los requerimientos computacionales: </span>[Versi√≥n PyTorch](https://pytorch.org/get-started/locally/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio # Reemplace en esta linea sin eliminar la expresi√≥n ! el comando que obtuvo en la pagina de PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaci√≥n versi√≥n de modelo Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versiones de los modelos para detecci√≥n de objetos\n",
    "\n",
    "ruta_model_yolo=input()\n",
    "if ruta_model_yolo=='v8n':\n",
    "    save='C:/Users/matrix/pruebayolo/yolov8n.pt'\n",
    "    url = 'https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt'#yolov8n\n",
    "    gdown.download(url,save,quiet=False)\n",
    "elif ruta_model_yolo=='v8s':\n",
    "    save='C:/Users/jemss/Workspace/yolov8s.pt'\n",
    "    url= 'https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt'#yolov8s\n",
    "    gdown.download(url,save,quiet=False)\n",
    "elif ruta_model_yolo=='v8m':\n",
    "    save='C:/Users/jemss/Workspace/yolov8m .pt'\n",
    "    url='https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt' #yolov8m\n",
    "    gdown.download(url,save,quiet=False)\n",
    "elif ruta_model_yolo=='v8l':\n",
    "    save='C:/Users/jemss/Workspace/yolov8l.pt'\n",
    "    url='https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt' #yolov8l\n",
    "    gdown.download(url,save,quiet=False)\n",
    "else:\n",
    "    save='C:/Users/jemss/Workspace/yolov8x.pt'\n",
    "    url='https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt' #yolov8x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_YOLO='yolov8n.pt'\n",
    "\n",
    "model_detection=YOLO(model_version_YOLO)\n",
    "\n",
    "PROJECT='deteccion' # Le permite asignar un nombre al directorio de inicio que contendr√° los experimentos de detecci√≥n de objetos, y debe estar entre comillas; se recomienda no utilizar espacios en el nombre.\n",
    "NAME='prueba_entrenamiento' # El nombre del experimento entrenamiento para deteccion de objetos debe ir entre comillas. Evite el uso de espacios al nombrar las carpetas; en su lugar, utilice alg√∫n formato de nombres como camelCase, snake_case o PascalCase.\n",
    "TASK='detect' # Define la tarea principal que desea realizar con el modelo YOLO v8; para este caso, la deteccion de objetos implica identificar la ubicaci√≥n y la clase de objetos en una imagen o un flujo de video.\n",
    "IMGSZ=640 # Establezca las dimensiones en p√≠xeles de la imagen de entrada. Puede especificarlo como un n√∫mero entero, como imgsz a 640 para obtener un cuadrado perfecto, o como una tupla, como imgsz=(640,480) para establecer dimensiones espec√≠ficas de ancho y alto. Se recomienda ajustar este valor seg√∫n el tama√±o del objeto que desea detectar. Para la detencci√≥n de objetos peque√±os, se recomienda aumentar el valor a m√°s de 640 p√≠xeles para obtener una resoluci√≥n m√°s alta.\n",
    "DATA=\"C:/Users/matrix/pruebayolo/data/dataset.yaml\" # ¬†Le permite indicar la ruta al archivo que contiene los metadatos que utilizara el modelo detecci√≥n de objetos y su configuraci√≥n en formato YAML. Si especifica el valor data=None, el conjunto de datos coco128-seg.yaml se utiliza de forma predeterminada; de lo contrario, escriba la ruta al archivo YAML entre comillas utilizando barras diagonales (/) en lugar de barras invertidas (\\).\n",
    "EPOCHS=30 # Establezca el n√∫mero de √©pocas del modelo YOLO v8 en la tarea de deteccion. Este valor representa el n√∫mero total de iteraciones en todo el conjunto de datos de entrenamiento. Se recomienda experimentar con este par√°metro dependiendo de la cantidad de im√°genes disponibles. Si tiene un conjunto de datos grande, considere aumentar este valor por encima de 30 para obtener mejores resultados. Por otro lado, establecer epochs=None har√° que el modelo contin√∫e entren√°ndose hasta que la p√©rdida de validaci√≥n deje de mejorar.\n",
    "BATCH=-1 # Define la cantidad de im√°genes procesadas simult√°neamente en una iteraci√≥n del modelo YOLO v8 en la deteccion. El valor predeterminado es 16; se recomienda establecerlo en -1 para aprovechar AutoBatch, que ajusta autom√°ticamente el tama√±o del lote para optimizar el rendimiento, evitar problemas de memoria y maximizar la eficiencia del entrenamiento. Si desea personalizarlo, exprese el valor del par√°metro como un n√∫mero entero.\n",
    "OPTIMIZER='auto' # Define el algoritmo de optimizaci√≥n para el modelo de deteccion YOLO. Su elecci√≥n ajusta los pesos del modelo durante el entrenamiento y es crucial para la velocidad y rendimiento. Puede tomar valores como 'SGD', 'Adam', 'Adamax', 'AdamW', 'NAdam', 'RAdam', 'RMSProp' y 'auto', este √∫ltimo selecciona autom√°ticamente el optimizador m√°s adecuado a la tarea segmentaci√≥n de objetos.\n",
    "WORKERS=1 # Especifica el n√∫mero de hilos de trabajo para la carga de datos en el modelo deteccion YOLO. Es recomendable utilizar un n√∫mero de subprocesos que se ajusten al n√∫mero de n√∫cleos del CPU disponibles en el sistema.\n",
    "DEVICE= 'cpu' # Especifica el dispositivo de ejecuci√≥n para la versi√≥n del modelo yolov8 en la operaci√≥n de deteccion. Puede seleccionar entre CPU o GPU. Si no dispone de una GPU con Cuda, se recomienda utilizar la CPU mediante el par√°metro device='cpu'. En caso de contar con Cuda, puede especificar una GPU con device='cuda:0'; el n√∫mero representa el identificador de la GPU disponible en el sistema. Tambi√©n es posible utilizar m√∫ltiples GPUs mediante device='cuda:0,1,2'.\n",
    "PLOTS=True # Utilice valores booleanos (Verdadero o Falso) para controlar la generaci√≥n de gr√°ficos que permite visualizar y monitorear la p√©rdida y la precisi√≥n durante el entrenamiento de deteccion de objetos. Establecer plots=True activara la funci√≥n; si desea desactivarla, establezca el valor del hiperpar√°metro en False.\n",
    "SAVE=True # Cuando se establece en True, el modelo guarda puntos de control peri√≥dicamente durante el entrenamiento de deteccion. Se recomienda tener cuidado al establecer este valor en Verdadero, ya que est√° relacionado con el hiperpar√°metro SAVE_PERIOD; si establece el valor del hiperparametro a False, la funci√≥n Save_Period se desactivar√°.\n",
    "SAVE_PERIOD=-1 # Se utiliza para especificar con qu√© frecuencia se guardan los puntos de control durante el entrenamiento de deteccion. Si se establece en un valor mayor que 0, el modelo guardar√° puntos de control cada n√∫mero especificado de √©pocas. Sin embargo, si save_period se establece en -1, significa que la funci√≥n esta deshabilitada.\n",
    "PATIENCE=30 # Representa el n√∫mero esperado de √©pocas durante el entrenamiento de deteccion. Si no se observa mejora en el conjunto de validaci√≥n dentro de un per√≠odo espec√≠fico, se detiene el proceso. Esta t√©cnica de parada temprana se utiliza para evitar el sobreajuste del modelo. Se recomienda ajustar este hiperpar√°metro en funci√≥n de la duraci√≥n esperada del entrenamiento.\n",
    "VERBOSE=True # Se utiliza para controlar el n√∫mero de impresiones durante la ejecuci√≥n del entrenamiento de deteccion. Para suprimir la salida de informaci√≥n b√°sica √∫nicamente, debe establecer el valor del hiperpar√°metro en False, pero si desea una salida de progreso m√°s detallada, establezca el valor en True.\n",
    "RECT= False # Habilita la formaci√≥n rectangular en cada lote, redimensionando las im√°genes para que todas tengan la misma forma rectangular. Puedes establecerlo en True si tu conjunto de datos es extenso y deseas acelerar el tiempo de entrenamiento en la deteccion de objetos. De lo contrario, si se establece en False el modelo se entrena en el orden normal procesando todos los datos de un lote antes de pasar al siguiente lote.\n",
    "COS_LR=True # Reemplaza el decaimiento escalonado predeterminado de YOLOv8, que reduce la tasa de aprendizaje en ciertas √©pocas, con el decaimiento escalonado cos_lr. Este ajusta la tasa de aprendizaje seg√∫n las √©pocas restantes y la tasa de aprendizaje inicial, proporcionando una disminuci√≥n m√°s suave. Establezca este hiperpar√°metro en True para una reducci√≥n gradual de la tasa de aprendizaje, de lo contrario establezca en False.\n",
    "FRACTION= 1.0 # Controla la fracci√≥n del conjunto de datos que se utilizara para el entrenamiento de deteccion. Debes establecer este par√°metro entre 0.0 y 1.0. Por defecto, cuando es 1.0, se emplea el 100% de las im√°genes disponibles en el conjunto de datos.\n",
    "EXIST_OK=False # Controla la sobreescritura de un experimento de deteccion existente. Cuando se establece en False, el sistema no sobrescribir√°, en su lugar devolver√° una ruta incrementada. Esto es √∫til para prevenir la sobreescritura accidental de experimentos anteriores. Para activar la funci√≥n, asigna el valor True.\n",
    "\n",
    "model_detection.train(project=PROJECT,name=NAME, task=TASK, data=DATA, imgsz=IMGSZ, epochs=EPOCHS, batch=BATCH, workers=WORKERS, device=DEVICE, plots=PLOTS, verbose=VERBOSE, rect=RECT, cos_lr=COS_LR, optimizer=OPTIMIZER, fraction=FRACTION, patience=PATIENCE, exist_ok=EXIST_OK\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraci√≥n para la exportaci√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n se muestra una tabla de referencia para exportar un modelo YOLOv8 entrenado en la tarea deteccion de objetos. Tenga en cuenta que configurar el par√°metro de `format` es fundamental para el proceso de exportaci√≥n. Antes de continuar, aseg√∫rese de verificar y ajustar estos valores a sus requisitos espec√≠ficos: \n",
    "| Formatos                                                             | Asignaci√≥n | Extensi√≥n                     | Hyperpar√°metros                                            | Descripci√≥n                                        |\n",
    "|--------------------------------------------------------------------|-------------------|---------------------------|-----------------------------------------------------|----------------------------------------------------|\n",
    "| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | -                                                   | Modelo en formato PyTorch                           |\n",
    "| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | \"torchscript\"     | `yolov8n.torchscript`     | `imgsz`, `optimize`                                 | Simplifica la implementaci√≥n de modelos PyTorch en entornos de producci√≥n y aplicaciones eficientes, mejorando la portabilidad y el rendimiento al permitir ejecutar una representaci√≥n intermedia en entornos sin Python.                       |\n",
    "| [ONNX](https://onnx.ai/)                                           | \"onnx\"            | `yolov8n.onnx`            | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     | Desarrollado para promover la interoperabilidad, la optimizaci√≥n del hardware y la colaboraci√≥n entre comunidades, al tiempo que responde a la necesidad de portabilidad de los modelos entre distintos marcos y herramientas de aprendizaje autom√°tico.                              |\n",
    "| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | \"openvino\"        | `yolov8n_openvino_model/` | `imgsz`, `half`, `int8`                             | Elaborado para promover la interoperabilidad, la optimizaci√≥n del hardware y el despliegue eficiente de modelos a trav√©s de diferentes marcos y herramientas de aprendizaje autom√°tico, con especial atenci√≥n a las plataformas de hardware Intel.                     |\n",
    "| [TensorRT](https://developer.nvidia.com/tensorrt)                  | \"engine\"          | `yolov8n.engine`          | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` | Permite promover la interoperabilidad, la optimizaci√≥n del hardware y la implantaci√≥n eficiente de modelos en distintos marcos y herramientas de aprendizaje autom√°tico, con especial atenci√≥n a las plataformas de hardware de NVIDIA.                     |\n",
    "| [CoreML](https://github.com/apple/coremltools)                     | \"coreml\"          | `yolov8n.mlpackage`       | `imgsz`, `half`, `int8`, `nms`                      | Posibilita promover la interoperabilidad, la optimizaci√≥n del hardware y el despliegue eficiente de modelos a trav√©s de diferentes marcos y herramientas de aprendizaje autom√°tico, con especial atenci√≥n a las plataformas de hardware de Apple.                            |\n",
    "| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | \"saved_model\"     | `yolov8n_saved_model/`    | `imgsz`, `keras`, `int8`                            | Empleado para guardar, compartir y desplegar modelos entrenados con TensorFlow. Vers√°til y facilita el despliegue en diversas plataformas como servidores, dispositivos m√≥viles, embebidos y navegadores.                     |\n",
    "| [TF Lite](https://www.tensorflow.org/lite)                         | \"tflite\"          | `yolov8n.tflite`          | `imgsz`, `half`, `int8`                             | Dise√±ado para el aprendizaje autom√°tico en dispositivos, TF Lite aborda restricciones clave como latencia, privacidad, conectividad, tama√±o y consumo de energ√≠a. Es esencial para desplegar modelos en dispositivos m√≥viles e integrados, ofreciendo una soluci√≥n ligera y eficiente.                           |\n",
    "| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | \"edgetpu\"         | `yolov8n_edgetpu.tflite`  | `imgsz`                                             | Utilizado para desplegar modelos de aprendizaje autom√°tico en el Edge TPU de TensorFlow. El Edge TPU es un peque√±o ASIC (Circuito Integrado Espec√≠fico de Aplicaci√≥n) dise√±ado por Google para ofrecer inferencias de aprendizaje autom√°tico de alto rendimiento en dispositivos de bajo consumo.                       |\n",
    "| [TF.js](https://www.tensorflow.org/js)                             | \"tfjs\"            | `yolov8n_web_model/`      | `imgsz`                                             | Facilita el despliegue de modelos de aprendizaje autom√°tico en navegadores web y Node.js, destacando la portabilidad y la facilidad de uso.                    |\n",
    "| [PaddlePaddle](https://github.com/PaddlePaddle)                    | \"paddle\"          | `yolov8n_paddle_model/`   | `imgsz`                                             | Utilizado para desplegar modelos en PaddlePaddle, una plataforma de aprendizaje profundo de c√≥digo abierto, paralela y distribuida que tiene su origen en la pr√°ctica industrial.                      |\n",
    "| [ncnn](https://github.com/Tencent/ncnn)                            | \"ncnn\"            | `yolov8n_ncnn_model/`     | `imgsz`, `half`                                     | Formato optimizado para plataformas m√≥viles, ofreciendo alto rendimiento. Puede incluir una estructura de archivo de modelo con informaci√≥n sobre capas, blobs de entrada y salida, y otros par√°metros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORTACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_det=\"C:/Users/jemss/runs/detect/Pruebadeteccion/weights/best.pt\"\n",
    "\n",
    "selected_model_det=YOLO(trained_model_det)\n",
    "\n",
    "FORMAT='onnx' # Seleccione el formato de exportaci√≥n del modelo deteccion, empleando la tabla previamente proporcionada; en la columna \"Asignaci√≥n\" se proporcionan las opciones para ajustar este valor.\n",
    "INT8=True # Establezca este par√°metro en True al utilizar la CPU y en False en caso contrario. La cuantificaci√≥n a INT8 mejora la eficiencia del modelo deteccion en cuanto a memoria y velocidad de inferencia, especialmente en hardware que admite esta precisi√≥n.\n",
    "HALF=False # Config√∫relo en True cuando use la GPU; en caso contrario, False. La cuantificaci√≥n a FP16 mejora la eficiencia de la memoria del modelo deteccion y la velocidad de inferencia, especialmente en hardware que admite precisi√≥n de punto flotante de 16 bits.\n",
    "IMGSZ=640 # Establezca las dimensiones en p√≠xeles de la imagen de entrada para la exportaci√≥n del modelo de deteccion. Puede especificarlo como un n√∫mero entero, por ejemplo, 640 para un cuadrado perfecto, o como una tupla, por ejemplo, (640, 480) para dimensiones espec√≠ficas de ancho y alto. Las im√°genes que ingreses al modelo despu√©s de la exportaci√≥n deben tener las mismas dimensiones espec√≠ficas que has configurado para adaptarse a los requisitos del escenario de despliegue.\n",
    "OPTIMIZE=False # Controla la optimizaci√≥n en modelos de detecci√≥n a TorchScript para su implementaci√≥n m√≥vil. Es importante destacar que esta funci√≥n puede resultar en un aumento significativo en el tama√±o del modelo exportado, lo cual puede no ser ideal para aplicaciones m√≥viles. Se configura con True para activar y False para desactivar.\n",
    "DYNAMIC=False # Controla la habilitaci√≥n de ejes din√°micos en modelos de deteccion, lo cual es particularmente √∫til para gestionar tama√±os de lote variables. Esta caracter√≠stica funciona bien en escenarios donde el tama√±o del lote puede cambiar durante la inferencia, como aplicaciones en tiempo real o de transmisi√≥n por secuencias. Los valores aceptados son Verdadero para habilitar la funci√≥n y Falso para deshabilitarla.\n",
    "SIMPLIFY=False # En la exportaci√≥n de modelos de deteccion a ONNX|TensorRT, este hiperpar√°metro personaliza la complejidad del modelo controlando la optimizaci√≥n, eliminando capas redundantes y reduciendo la precisi√≥n de los par√°metros. Se activa con True y se desactiva con False.\n",
    "OPSET=False # Especifica la versi√≥n del conjunto de operadores en ONNX al exportar el modelo deteccion desde marcos como PyTorch o TensorFlow. Si se deja en \"None\", ONNX utilizar√° autom√°ticamente la versi√≥n m√°s reciente disponible; para una versi√≥n espec√≠fica, asigne el n√∫mero entre comillas, por ejemplo, \"11\".\n",
    "WORKSPACE=4 # En la exportaci√≥n de modelos de deteccion a TensorRT, establece el tama√±o del espacio de trabajo en GB asignado para optimizar y preparar el modelo de red neuronal. Este espacio se utiliza durante la construcci√≥n del motor para lograr una ejecuci√≥n eficiente en hardware GPU mediante la biblioteca TensorRT.\n",
    "NMS=False # En la exportaci√≥n de modelos de deteccion a CoreML, controla la inclusi√≥n de la Supresi√≥n No M√°xima (NMS) en el modelo para eliminar cuadros delimitadores redundantes en la segmentaci√≥n de instancias y mejorar la precisi√≥n de las predicciones. Establecer 'NMS' en 'False' ignora NMS en los modelos CoreML exportados.  Este ajuste, configurable durante la exportaci√≥n del modelo YOLO, lo que permite a los usuarios optimizar la implementaci√≥n del modelo en una variedad de plataformas y dispositivos.\n",
    "KERAS= False # En la exportaci√≥n de modelos de deteccion a TF SavedModel y TF Lite, permite optimizar el despliegue en diversas plataformas y dispositivos. Incluye tambi√©n el formato del archivo, el dispositivo de ejecuci√≥n y la posibilidad de manejar m√∫ltiples etiquetas por caja. Establezca el valor del hiperpar√°metro en True si est√° familiarizado con Keras; de lo contrario, en False para excluir su uso en la exportaci√≥n.\n",
    "\n",
    "selected_model_det.export(format=FORMAT, imgsz=640, dynamic=False, simplify=False, opset=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_det='C:/Users/matrix/pruebayolo/DETECCI√ìN/Prueba_detecci√≥n/weights/best.pt'\n",
    "\n",
    "selected_model_det=YOLO(trained_model_det)\n",
    "\n",
    "NAME='Prueba_validaci√≥n_det' # El nombre del experimento de validaci√≥n para deteccion de objetos deben ir entre comillas. Evite el uso de espacios al nombrar las carpetas; en su lugar, utilice alg√∫n formato de nombres como camelCase, snake_case o PascalCase.\n",
    "DATA= 'C:/Users/matrix/pruebayolo/data/dataset.yaml' #Permite indicar la ruta al archivo que contiene los metadatos necesarios para el proceso de validaci√≥n, la ruta debe ser proporcionada entre comillas.\n",
    "SAVE_JSON=True # Si se configura como True, habilita la funcionalidad de guardar los resultados obtenidos de manera detallada del proceso de validaci√≥n en un formato estructurado JSON.\n",
    "IMGSZ=640 # Establece las dimensiones en p√≠xeles de la imagen de entrada para la validaci√≥n del modelo de deteccion. Puede ser un n√∫mero entero, como 640 para un cuadrado perfecto, o una tupla, como (640, 480), para dimensiones espec√≠ficas de ancho y alto. Se recomienda usar el mismo valor que utilizo durante el entrenamiento del modelo.\n",
    "BATCH=16 # Define la cantidad de im√°genes procesadas simult√°neamente en una iteraci√≥n para la validaci√≥n en detecciones de objetos. El valor predeterminado es 16; se recomienda establecerlo en -1 para utilizar AutoBatch, que ajusta autom√°ticamente el tama√±o del lote para optimizar el rendimiento y la eficiencia del entrenamiento, evitando problemas de memoria. Si desea personalizarlo, establezca el valor como un n√∫mero entero.\n",
    "SAVE_HYBRID=True # Activa la funci√≥n con True para guardar una versi√≥n h√≠brida de la etiqueta, incluyendo la original y predicciones adicionales. √ötil para el an√°lisis detallado del rendimiento del modelo deteccion durante la validaci√≥n; establezca en False para mostrar solo las predicciones.\n",
    "CONF=0.5 # Establece el umbral de confianza para la validaci√≥n de clases en la tarea de deteccion. Se recomienda un valor entre 0.5 y 0.10. Un umbral m√°s alto mejora la precisi√≥n pero reduce la frecuencia de predicciones, mientras que un umbral m√°s bajo aumenta la frecuencia pero disminuye la precisi√≥n. \n",
    "MAX_DET=10 # Toma como valor solo n√∫meros enteros. √çndica el l√≠mite de la cantidad m√°xima de objetos que el modelo intentara detectar en una imagen. Se recomienda establecer un valor alto para evitar perder detecciones relevantes.\n",
    "DEVICE='CPU' # Especifica el dispositivo de ejecuci√≥n para la prueba de validaci√≥n en la operaci√≥n de deteccion. Puede seleccionar entre CPU o GPU. Si no dispone de una GPU con Cuda, se recomienda utilizar la CPU mediante el par√°metro device='cpu'. En caso de contar con Cuda, puede especificar una GPU con device='cuda:0'; el n√∫mero representa el identificador de la GPU disponible en el sistema. Tambi√©n es posible utilizar m√∫ltiples GPUs mediante device='cuda:0,1,2'.\n",
    "PLOTS=True # Utilice valores booleanos (Verdadero o Falso) para controlar la generaci√≥n de gr√°ficos que permite visualizar y monitorear la p√©rdida y la precisi√≥n durante la validaci√≥n en la deteccion de objetos. Establecer plots=True activara la funci√≥n; si desea desactivarla, establezca el valor del hiperpar√°metro en False.\n",
    "RECT=False # Habilita la formaci√≥n rectangular en cada lote, redimensionando las im√°genes para que todas tengan la misma forma rectangular. Puedes establecerlo en True si tu conjunto de datos es extenso y deseas acelerar el tiempo de validaci√≥n en la deteccion de objetos. De lo contrario, si se establece en False el modelo se entrena en el orden normal procesando todos los datos de un lote antes de pasar al siguiente.\n",
    "IOU=0.6 # El umbral predeterminado para la supresi√≥n no m√°xima (NMS) en la validaci√≥n YOLO es 0,6. Este umbral de IoU (intersecci√≥n sobre uni√≥n) es fundamental para NMS porque determina el grado m√≠nimo de superposici√≥n requerido para que dos cuadros delimitadores se consideren el mismo objeto. Un umbral de IoU m√°s bajo hace que NMS sea m√°s conservador, mientras que un umbral de IoU m√°s alto permite que un NMS m√°s relajado evite eliminar los verdaderos positivos.\n",
    "\n",
    "selected_model_det.val(name=NAME, data=DATA, save_json=SAVE_JSON, imgsz=IMGSZ, batch=BATCH, save_hybrid=SAVE_HYBRID, conf=CONF, max_det=MAX_DET, device=DEVICE, plots=PLOTS, rect=RECT, iou=IOU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraci√≥n de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar m√∫ltiples fuentes de datos al realizar predicciones con el modelo, se requiere que se ajuste el par√°metro'source' a sus necesidades, tal como se indica en la siguiente tabla:\n",
    "| Fuentes          | Asignaci√≥n                             | Tipo             | Notas                                                           |\n",
    "| --------------- | ------------------------------------ | ----------------- | --------------------------------------------------------------- |\n",
    "| `image`           | 'image.jpg'                          | str or Path       | Archivo que contiene una √∫nica imagen.                                              |\n",
    "| `URL`             | 'https://ultralytics.com/images/bus.jpg' | str               | Direcci√≥n que especifica la ubicaci√≥n de una imagen en la web.                                                 |\n",
    "| `screenshot`      | 'screen'                             | str               | El sistema captura la imagen actualmente visible en la pantalla y la utiliza como entrada para el modelo.                                           |\n",
    "| `PIL`             | Image.open('im.jpg')                 | PIL.Image         | Utilizado para cargar im√°genes en formato HWC (altura, ancho, canales) con canales RGB (rojo, verde y azul) mediante la biblioteca Python Imaging Library (PIL).                                   |\n",
    "| `OpenCV`          | cv2.imread('im.jpg')                 | np.ndarray        | Permite la lectura de una imagen desde un archivo en formato HWC con canales BGR (azul, verde, rojo) utilizando la biblioteca OpenCV, almacenando la imagen como un array de NumPy.                    |\n",
    "| `numpy`           | np.zeros((640,1280,3))               | np.ndarray        | Genera un array de ceros con las dimensiones especificadas para un formato HWC con canales BGR, utilizando la biblioteca NumPy.                    |\n",
    "| `torch`           | torch.zeros(16,3,320,640)            | torch.Tensor      | Crea un tensor de ceros con las dimensiones especificadas para un formato HWC con canales RGB, empleando el framework PyTorch.               |\n",
    "| `CSV`             | 'sources.csv'                        | str or Path       | Archivo de texto que almacena las rutas a las im√°genes que se procesar√°n.   |\n",
    "| `video`          | 'video.mp4'                          | str or Path       | Proporciona acceso a un archivo de video √∫nico.                       |\n",
    "| `directory`      | 'path/'                              | str or Path       | Directorio que contiene m√∫ltiples archivos de imagen.               |\n",
    "| `glob`           | 'path/*.jpg'                         | str               | Permite acceder a varias im√°genes en un directorio usando expresiones de coincidencia de patrones. |\n",
    "| `YouTube`        | 'https://youtu.be/LNwODJXcvt4'       | str               | Facilita el acceso a videos desde la plataforma YouTube.                                         |\n",
    "| `stream`         | 'rtsp://example.com/media.mp4'      | str               | Permite la conexi√≥n a flujos de video o audio en tiempo real mediante protocolos como RTSP, RTMP, TCP o IP, ya sea a trav√©s de internet o una red local. |\n",
    "| `multi-stream`   | 'list.streams'                       | str or Path       | Se utiliza para transmitir varios flujos de medios simult√°neamente, permitiendo el procesamiento y an√°lisis paralelo de m√∫ltiples flujos de medios. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatos para las im√°genes: \n",
    "\n",
    "| Image Suffixes | Reference                           |\n",
    "| --------------- | ----------------------------------- |\n",
    "| .bmp            | [Microsoft BMP File Format](https://docs.fileformat.com/es/image/bmp/)           |\n",
    "| .dng            | [Adobe DNG](https://docs.fileformat.com/es/image/dng/)                           |\n",
    "| .jpeg           | [JPEG](https://docs.fileformat.com/es/image/jpeg/)                                |\n",
    "| .jpg            | [JPEG](https://docs.fileformat.com/es/image/jpeg/)                                |\n",
    "| .mpo            | [Multi Picture Object](https://docs.fileformat.com/es/image/mpo/)                |\n",
    "| .png            | [Portable Network Graphics](https://docs.fileformat.com/es/image/png/)           |\n",
    "| .tif            | [Tag Image File Format](https://docs.fileformat.com/es/image/tiff/)               |\n",
    "| .tiff           | [Tag Image File Format](https://docs.fileformat.com/es/image/tiff/)               |\n",
    "| .webp           | [WebP](https://docs.fileformat.com/es/image/webp/)                                |\n",
    "| .pfm            | [Portable FloatMap](https://docs.fileformat.com/font/pfm/)                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatos para los videos: \n",
    "\n",
    "| Video Suffixes | Reference                           |\n",
    "| -------------- | ----------------------------------- |\n",
    "| .asf           | [Advanced Systems Format](https://docs.fileformat.com/es/video/asf/)             |\n",
    "| .avi           | [Audio Video Interleave](https://docs.fileformat.com/es/video/avi/)              |\n",
    "| .gif           | [Graphics Interchange Format]()          |\n",
    "| .m4v           | [MPEG-4 Part 14](https://docs.fileformat.com/es/video/m4v/)                      |\n",
    "| .mkv           | [Matroska](https://docs.fileformat.com/es/video/mkv/)                            |\n",
    "| .mov           | [QuickTime File Format](https://docs.fileformat.com/es/video/mov/)               |\n",
    "| .mp4           | [MPEG-4](https://docs.fileformat.com/es/video/mp4/)          |\n",
    "| .mpeg          | [MPEG-1](https://docs.fileformat.com/es/video/mpeg/)                       |\n",
    "| .mpg           | [MPEG-1](https://docs.fileformat.com/es/video/mpeg/)                       |\n",
    "| .ts            | [MPEG Transport Stream](https://docs.fileformat.com/es/video/ts/)               |\n",
    "| .wmv           | [Windows Media Video](https://docs.fileformat.com/es/video/wmv/)                 |\n",
    "| .webm          | [WebM Project](https://docs.fileformat.com/es/video/webm/)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_det='C:/Users/matrix/pruebayolo/DETECCI√ìN/Prueba_detecci√≥n/weights/best.pt'\n",
    "\n",
    "selected_model_det=YOLO(trained_model_det)\n",
    "\n",
    "NAME='PruebaPrediccion' # El nombre del experimento de predicci√≥n para deteccion de objetos debe ir entre comillas. Evite el uso de espacios al nombrar las carpetas; en su lugar, utilice alg√∫n formato de nombres como camelCase, snake_case o PascalCase.\n",
    "SOURCE='' # Establezca el origen de datos que el modelo de detecci√≥n utilizar√° para realizar predicciones. Configure el valor de este hiperpar√°metro seg√∫n la tabla proporcionada anteriormente.\n",
    "MAX_DET=10 # Toma como valor solo n√∫meros enteros. √çndica el l√≠mite de la cantidad m√°xima de objetos que el modelo intentara predecir en una imagen. Se recomienda establecer un valor alto para evitar perder detecciones relevantes.\n",
    "IMGSZ=640 # Establezca las dimensiones en p√≠xeles de la imagen de entrada durante la predicci√≥n en tareas de detecci√≥n. Puede ser un n√∫mero entero, como 640 para un cuadrado perfecto, o una tupla, como (640, 480), para dimensiones espec√≠ficas de ancho y alto. Se recomienda utilizar los mismos valores utilizados durante el entrenamiento del modelo para mantener la coherencia en la inferencia.\n",
    "CONF=0.1 # Establece el umbral de confianza durante el proceso de predicci√≥n en la tarea de detecci√≥n. Se recomienda establecer el valor hiperpar√°metro entre 0.5 y 0.10. Un umbral m√°s alto mejora la precisi√≥n pero reduce la frecuencia de predicciones, mientras que un umbral m√°s bajo aumenta la frecuencia pero disminuye la precisi√≥n en la inferencia.\n",
    "LINE_WIDTH= None # Determina el grosor en p√≠xeles de los cuadros delimitadores que rodean los objetos detectados por el modelo. Puede establecer el grosor de la l√≠nea como un n√∫mero entero en el que, a mayor valor, la l√≠nea ser√° m√°s gruesa, tambi√©n puede utilizar como valor None para que el grosor se ajuste de forma automatizada, proporcionando una l√≠nea proporcional al tama√±o de la imagen.\n",
    "VISUALIZE=False # Determina si las caracter√≠sticas del modelo de detecci√≥n deben mostrarse durante la predicci√≥n. Establecer esto en True permite que las caracter√≠sticas se muestren como mapas intermedias, lo que hace que el modelo sea m√°s f√°cil de entender. Si se establece en False, no se mostrar√°n las caracter√≠sticas del modelo. \n",
    "IOU=0.7 # El umbral predeterminado para la supresi√≥n no m√°xima (NMS) en la validaci√≥n YOLO es 0,7. Este umbral de IoU (intersecci√≥n sobre uni√≥n) es fundamental para NMS porque determina el grado m√≠nimo de superposici√≥n requerido para que dos cuadros delimitadores se consideren la misma detecci√≥n. Un umbral de IoU m√°s bajo hace que NMS sea m√°s conservador, mientras que un umbral de IoU m√°s alto permite que un NMS m√°s relajado evite eliminar los verdaderos positivos.\n",
    "DEVICE='cpu' # Especifica el dispositivo de ejecuci√≥n para la prueba de predicci√≥n en la operaci√≥n de detecci√≥n. Puede seleccionar entre CPU o GPU. Si no dispone de una GPU con Cuda, se recomienda utilizar la CPU mediante el par√°metro device='cpu'. En caso de contar con Cuda, puede especificar una GPU con device='cuda:0'; el n√∫mero representa el identificador de la GPU disponible en el sistema. Tambi√©n es posible utilizar m√∫ltiples GPUs mediante device='cuda:0,1,2'.\n",
    "VID_STRIDE=False # Controla la velocidad de los fotogramas durante el proceso de predicci√≥n en v√≠deos o secuencias de tiempo real. Al establecerlo en True el modelo se adapta a la velocidad de fotogramas especificada por la fuente de v√≠deo, procesando cada fotograma individualmente. Para desactivar esta funci√≥n indique como valor False.\n",
    "STREAM_BUFFER=False # Controla el almacenamiento en b√∫fer de los fotogramas para la detecci√≥n. Si es True, se almacenan todos los fotogramas para el procesamiento en tiempo real de v√≠deos o transmisiones en directo; si es False, devuelve el fotograma m√°s reciente.\n",
    "SAVE_FRAMES=False # Controla la captura y almacenamiento de los fotogramas predichos por el modelo de detecci√≥n. Con True, se guardar√°n todos los fotogramas individuales predichos; con False, no se realizar√° el almacenamiento de los fotogramas.\n",
    "AUGMENT=False # Aplica transformaciones a las im√°genes de entrada, tales como giros, rotaciones, recortes y cambios de color, para diversificar los datos y mejorar la predicci√≥n en la detecci√≥n. Establecer en True para activar la funci√≥n, False para desactivar.\n",
    "CLASSES=None # Filtra los resultados por clase durante la predicci√≥n en detecci√≥n. Puede establecerse con un solo ID de clase o una lista de ID de clases para incluir espec√≠ficamente esas clases en los resultados. Por ejemplo, \"classes=0\" o \"classes=[0, 1]\", tambi√©n puede establecer el valor a None que tomara todas las clases configuradas en el dataset.\n",
    "SAVE_CROP=True # Determina si se deben guardar im√°genes recortadas con los resultados durante la predicci√≥n en la detecci√≥n. Al establecerlo en \"False\", las im√°genes recortadas no se guardar√°n, lo que reduce el tama√±o del archivo. Con el valor \"True\", se guardar√°n las im√°genes recortadas correspondientes a las √°reas detectadas.\n",
    "SHOW=False # Determina si se deben mostrar las im√°genes o v√≠deos detectados durante la predicci√≥n. Al establecerlo en \"True\", permite la visualizaci√≥n de las predicciones en el mismo entorno, proporcionando una representaci√≥n visual de los resultados. Si se establece en \"False\", las predicciones no se mostrar√°n. \n",
    "\n",
    "\n",
    "selected_model_det.predict(name=NAME, source=SOURCE,conf=CONF, save_txt=True, max_det=MAX_DET, line_width=LINE_WIDTH, visualize=VISUALIZE, imgsz=IMGSZ, save=SAVE, iou=IOU, device=DEVICE, vid_stride=VID_STRIDE, stream_buffer=STREAM_BUFFER, classes=CLASSES,  save_crop=SAVE_CROP, show=SHOW, save_frames= SAVE_FRAMES, save_json=True                  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funci√≥n para √°rea da√±ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen: imagen_402, √Årea total da√±ada: 37649.059515\n"
     ]
    }
   ],
   "source": [
    "#Por imagen\n",
    "\n",
    "def calcular_area_caja(bbox):\n",
    "    x_min, y_min, ancho, alto = bbox\n",
    "    area = ancho * alto\n",
    "    return area\n",
    "\n",
    "def calcular_total_caja(coordenadas):\n",
    "    area_da√±o = 0\n",
    "\n",
    "    for bbox_info in coordenadas:\n",
    "        bbox = bbox_info[\"bbox\"]\n",
    "        area_da√±o += calcular_area_caja(bbox)\n",
    "\n",
    "    return area_da√±o\n",
    "\n",
    "\n",
    "with open('C:/Users/matrix/detect/Prueba_validaci√≥n2/predictions.json', 'r') as json_prediccion:\n",
    "    resultado_prediccion_json = json.loads(json_prediccion.read())\n",
    "\n",
    "image_id = \"imagen_402\"\n",
    "coordenadas_img = []\n",
    "\n",
    "for item in resultado_prediccion_json:\n",
    "    if item[\"image_id\"] == image_id:\n",
    "        bbox = item[\"bbox\"]\n",
    "        coordenadas_img.append({\"bbox\": bbox})\n",
    "\n",
    "# Calcular y mostrar el √°rea da√±ada para la imagen deseada\n",
    "area_da√±ada_total = calcular_total_caja(coordenadas_img)\n",
    "print(f\"Imagen: {image_id}, √Årea total da√±ada: {area_da√±ada_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen: imagen_402, √Årea da√±ada: 37649.059515\n",
      "Imagen: imagen_403, √Årea da√±ada: 67204.984116\n",
      "Imagen: imagen_404, √Årea da√±ada: 21516.70767\n",
      "Imagen: imagen_405, √Årea da√±ada: 137839.98284500002\n",
      "Imagen: imagen_406, √Årea da√±ada: 180025.115365\n",
      "Imagen: imagen_407, √Årea da√±ada: 17813.157057999997\n",
      "Imagen: imagen_408, √Årea da√±ada: 57343.516874\n",
      "Imagen: imagen_409, √Årea da√±ada: 71888.827672\n",
      "Imagen: imagen_410, √Årea da√±ada: 415811.01618499996\n",
      "Imagen: imagen_411, √Årea da√±ada: 90691.62346500003\n",
      "Imagen: imagen_412, √Årea da√±ada: 103829.21065799998\n",
      "Imagen: imagen_413, √Årea da√±ada: 48190.903658\n",
      "Imagen: imagen_414, √Årea da√±ada: 104460.513679\n",
      "Imagen: imagen_415, √Årea da√±ada: 60448.14294799999\n",
      "Imagen: imagen_416, √Årea da√±ada: 165503.248478\n",
      "Imagen: imagen_417, √Årea da√±ada: 1233.5364\n",
      "Imagen: imagen_418, √Årea da√±ada: 130774.435068\n",
      "Imagen: imagen_419, √Årea da√±ada: 377033.152255\n",
      "Imagen: imagen_420, √Årea da√±ada: 137973.643683\n",
      "Imagen: imagen_421, √Årea da√±ada: 37001.564263\n",
      "Imagen: imagen_422, √Årea da√±ada: 190242.033536\n",
      "Imagen: imagen_423, √Årea da√±ada: 34533.608435\n",
      "Imagen: imagen_424, √Årea da√±ada: 123663.922554\n",
      "Imagen: imagen_425, √Årea da√±ada: 295285.777351\n",
      "Imagen: imagen_426, √Årea da√±ada: 9927.217021999999\n",
      "Imagen: imagen_427, √Årea da√±ada: 183596.806314\n",
      "Imagen: imagen_428, √Årea da√±ada: 146577.69014299999\n",
      "Imagen: imagen_429, √Årea da√±ada: 76853.581803\n",
      "Imagen: imagen_430, √Årea da√±ada: 296784.6789969999\n",
      "Imagen: imagen_431, √Årea da√±ada: 60114.48669600001\n",
      "Imagen: imagen_432, √Årea da√±ada: 66213.115185\n",
      "Imagen: imagen_433, √Årea da√±ada: 158277.460555\n",
      "Imagen: imagen_434, √Årea da√±ada: 331564.061999\n",
      "Imagen: imagen_435, √Årea da√±ada: 5846.8777\n",
      "Imagen: imagen_436, √Årea da√±ada: 368344.38429099997\n",
      "Imagen: imagen_437, √Årea da√±ada: 222765.69207699999\n",
      "Imagen: imagen_438, √Årea da√±ada: 718467.619509\n",
      "Imagen: imagen_439, √Årea da√±ada: 90958.15666600001\n",
      "Imagen: imagen_440, √Årea da√±ada: 212978.185119\n",
      "Imagen: imagen_441, √Årea da√±ada: 79028.882194\n",
      "Imagen: imagen_442, √Årea da√±ada: 9994.815344\n",
      "Imagen: imagen_443, √Årea da√±ada: 89884.21686\n",
      "Imagen: imagen_444, √Årea da√±ada: 37286.874539\n",
      "Imagen: imagen_445, √Årea da√±ada: 78427.360941\n",
      "Imagen: imagen_446, √Årea da√±ada: 224723.730912\n",
      "Imagen: imagen_447, √Årea da√±ada: 84548.064232\n",
      "Imagen: imagen_448, √Årea da√±ada: 75515.32049900002\n",
      "Imagen: imagen_449, √Årea da√±ada: 70476.171009\n"
     ]
    }
   ],
   "source": [
    "#Todas las imagenes \n",
    "\n",
    "def calcular_area(bbox):\n",
    "    x_min, y_min,ancho, alto = bbox\n",
    "    area = ancho * alto\n",
    "    return area\n",
    "\n",
    "def calcular_area_danada(coordenadas):\n",
    "    area_danada_total = 0\n",
    "\n",
    "    for bbox_info in coordenadas:\n",
    "        bbox = bbox_info[\"bbox\"]\n",
    "        area_danada_total += calcular_area(bbox)\n",
    "\n",
    "    return area_danada_total\n",
    "\n",
    "coordenadas_por_imagen = {}\n",
    "\n",
    "with open('C:/Users/matrix/detect/Prueba_validaci√≥n2/predictions.json', 'r') as json_data:\n",
    "    results = json.loads(json_data.read())\n",
    "\n",
    "for item in results:\n",
    "    image_id = item[\"image_id\"]\n",
    "    bbox = item[\"bbox\"]\n",
    "\n",
    "    if image_id not in coordenadas_por_imagen:\n",
    "        coordenadas_por_imagen[image_id] = []\n",
    "\n",
    "    coordenadas_por_imagen[image_id].append({\"bbox\": bbox})\n",
    "\n",
    "# Calcular el √°rea da√±ada para cada imagen\n",
    "for image_id, bbox_list in coordenadas_por_imagen.items():\n",
    "    area_danada = calcular_area_danada(bbox_list)\n",
    "    print(f\"Imagen: {image_id}, √Årea da√±ada: {area_danada}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
